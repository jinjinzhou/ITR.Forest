Model.Specification$data <- dat
Model.Specification$split.var <- split.var
Model.Specification$ctg <- ctg
Model.Specification$col.y <- col.y
Model.Specification$col.trt <- col.trt
Model.Specification$col.prtx <- col.prtx
out$Model.Specification <- Model.Specification
return(out)
}
# ------------------------------------------------------------------
# THIS senddown FUNCTION IS WRITTEN FOR THE VIARIABE IMPORTANCE PART
# USING RANDOM FORESTS
# ------------------------------------------------------------------
send.down.VI.ITR <- function(dat.new, tre, col.y, col.trt, col.prtx, ctg=NA, n0=5, revise.tree=T,depth=1)
{
node.dat <- rep(0, nrow(dat.new))   		# COLUMNS CAN BE ADDED TO DATA
cut.point <- as.vector(tre$cut.2)
cut.direct <- as.vector(tre$cut.1)
split.var <- as.numeric(as.vector(tre$var))
y <- dat.new[, col.y]    #Changed from y<-dat.new[, col.y] to y<-dat.new[, "y"]
trt <- dat.new[, col.trt]
prtx <- dat.new[,col.prtx]
nd <- dim(tre)[1]
tre0 <- tre # REVISED TREE
tre0$n.test <- rep(NA, nrow(tre))
tre0$score.test <- rep(NA, nrow(tre)) # COLUMNS CAN BE ADDED TO TREE
i <- 1
zz <- rep(0,nrow(dat.new))
while (i <= nrow(tre0)){
node.i <- tre0$node[i]
in.node <- (node.dat == node.i)
y0 <- y[in.node]
trt0 <- trt[in.node]
prtx0 <- prtx[in.node]
dat0 <- data.frame(y=y0, trt=trt0, prtx=prtx0)
n.0 <- length(y0)
tre0$n.test[i] <- n.0
t2 <- NA
if (!is.na(split.var[i])){
x.split <- dat.new[,split.var[i]];
cut <- cut.point[i]
cut.d <- cut.direct[i]
if (!is.element(split.var[i], ctg)) {
cut1 <- as.numeric(cut)
l.nd <- node.dat[in.node & x.split <= cut1]
r.nd <- node.dat[in.node & x.split > cut1]
z <- sign(x.split[in.node] <= cut1)
node.dat[in.node & x.split <= cut1] <- paste(l.nd, 1, sep="")
node.dat[in.node & x.split >  cut1] <- paste(r.nd, 2, sep="")
if(i <= depth){
if(cut.d=="l") {
zz[in.node & x.split <= cut1] <- 1
} else {
zz[in.node & x.split > cut1] <- 1
}
}
}
else {
cut1 <- unlist(strsplit(as.character(cut), split=" "))
l.nd <- node.dat[in.node & is.element(x.split, cut1)]
r.nd <- node.dat[in.node & !is.element(x.split, cut1)]
z <- sign(is.element(x.split[in.node], cut1))
node.dat[in.node & is.element(x.split, cut1)] <- paste(l.nd, 1, sep="")
node.dat[in.node & !is.element(x.split, cut1)] <- paste(r.nd, 2, sep="")
}
t2 <- itrtest2(dat0, z, n0=n0)
tre0$score.test[i] <- t2
}
if (is.na(t2) && revise.tree) {
node.rm <-  de(node.i, tre0)
tre0 <- tre0[!is.element(tre0$node, node.rm), ]
tre0[tre0$node==node.i, c("var", "vname", "cut.1", "cut.2", "score")] <- NA
}
i <- i+1
}
out  <- list(tre0=tre0,score=itrtest2(dat.new, zz, n0=n0))
return(out)
}
# =====================================================================
# FUNCTION Variable.Importance() COMPUTE VARIABLE IMPORTANCE MEASURES
# =====================================================================
# RF.fit = MUST BE AN OBJECT OR OUTPUT FROM FUNCTION Build.RF.IT() OR combine.RF()
# sort = OPTION TO SORT THE RESULTANT VI MEASURES
# truncate.zeros= OPTION TO TRUNCATE <= 0 VI MEAURESN TO 0.
Variable.Importance.ITR <- function(RF.fit, n0=2, sort=T, details=F, truncate.zeros=T,depth=1){
trees <- RF.fit$TREES
id.boots <- RF.fit$ID.Boots.Samples
# ARGUMETNS FOR MODEL SPECIFICATION
Model.Specification <- RF.fit$Model.Specification
dat0 <- Model.Specification$data
col.y <- Model.Specification$col.y
col.trt <- Model.Specification$col.trt
col.prtx <- Model.Specification$col.prtx
split.var <- Model.Specification$split.var
ctg <- Model.Specification$ctg
vnames <- colnames(dat0)[split.var]
#
ntree <- length(trees)
p <- length(split.var)
VI <- rep(0, p)
for (b in 1:ntree){
id.b <- id.boots[[b]]
dat.oob <- dat0[-sort(unique(id.b)), ]
n.oob <- nrow(dat.oob)
tre.b <- trees[[b]]
########## NOTE THAT revise.tree=T HERE! ##########
out0.b <- send.down.VI.ITR(dat.new=dat.oob, tre=tre.b, col.y=col.y, col.trt=col.trt, col.prtx=col.prtx, ctg=ctg, n0=n0, revise.tree=T,depth=1)
tre0.b <- out0.b$tre0
if (nrow(tre0.b) > 1) {						### AVOID NULL TREES
Xs.b <- sort(unique(na.omit(tre0.b$var)))
G.oob <- out0.b$score
for (j in 1:p) {
if (details) print(j)
G.j <- G.oob
col.xj <- split.var[j]
if (is.element(col.xj, Xs.b)){
x.j <- dat.oob[, col.xj]
dat.permuted <- dat.oob
dat.permuted[ , col.xj] <- x.j[sample(1:n.oob,n.oob, replace=F)]
########## NOTE THAT revise.tree=F HERE! ##########
out0.bj <- send.down.VI.ITR(dat.new=dat.permuted, tre=tre0.b, col.y=col.y, col.trt=col.trt, col.prtx=col.prtx, ctg=ctg, n0=n0, revise.tree=F,depth=1)
tre0.bj <- out0.bj$tre0
G.j <- ifelse(nrow(tre0.bj) ==1, G.oob, out0.bj$score)
}
if (G.j > G.oob) G.j <- G.oob
##################### PREVENTS NEGATIVE IMPORTANCE VALUES
VI[j] <- VI[j] + (G.oob - G.j)/G.oob
}
}
}
if (truncate.zeros) VI[VI <0] <- 0  		####### IS THIS STEP NECESSARY? NOPE.
names(VI) <- vnames
if (sort) VI <- sort(VI, decreasing=T)
VI<-VI/sum(VI)
return(VI)
}
# =====================================================================
# FUNCTION plot.VI() PLOTS VARIABLE IMPORTANCE MEASURES USING bar.plot
# =====================================================================
plot.VI <- function(VI, filename=NULL, horizontal=T, rain.bow=T)
{
library(RColorBrewer)
if (!is.null(filename)) postscript(file=filename, horizontal=horizontal)
par(mfrow=c(1, 1), mar = c(7, 4, 7, 4));
require(grDevices)
p <- length(VI)
color0 <- gray(0:(p - 1)/(p - 1))
if (rain.bow) color0 <- brewer.pal(p, "YlOrRd")
barplot(VI, col=color0, names.arg = names(VI), ylab="Importance (Proportion)", xlab="Variable",
cex.names = 1.2,  las=3);  # TO HAVE VERTICAL AXIS LABELING
text(x=Variable, y=VI, labels=VI, pos=3, xpd=NA)
title(main = list("Variable Importance Rank",  font = 4, cex = 1.4));
if (!is.null(filename)) dev.off()
}
# ==============================================
# FUNCTION rdat() SIMULATES A SIMPLE DATA SET
# n is sample size
# K is used to generate covarates as sample(1:K, n, replace=T)/K
# assume 5 covariates X1 and X2 are the interaction term
# y = beta0 + beta1*trt + beta2*I(x1<=cut1) + beta3*I(x2<=cut2) + beta4*I(x1<=cut1)*trt + beta5*I(x2<=cut2)*trt + rnorm(0,1)
# ==============================================
rdat <- function(n=100, K =50,
beta1=2, beta2=2,
sigma=1, cut1=.5, cut2=.5, depth=1)
{
trt <- sample(c(0,1), n, replace=T)
#### Generate Covariates
for (j in 1:4) {
assign(paste("x", j, sep=""),sample(1:K, n, replace=T)/K)
}
###
if(depth==1){
mean <- 2 + 2*sign(x1<=cut2) + beta1*sign(x1<=cut1)*trt + beta2*sign(x1>cut1)*(1-trt)
##### Output
}else{
mean <- 2 + 2*sign(x1<=cut2) + beta1*sign(x1>=0.3 & x3>=0.1)*trt + beta2*(1-sign(x1>=0.3 & x3>=0.1))*(1-trt)
}
y <- mean + rnorm(n, mean=0, sd=sigma)
data.frame(x1=x1, x2=x2, x3=x3, x4=x4, y=y, trt=trt,prtx=rep(0.5,n))
}
# ==============================================
# FUNCTION gdataM() SIMULATES A SIMPLE DATA SET
# n is sample size
# NX is the number of predictors.
# c is the non-trivial benefit
# assume x1 > 0.3 and x3 > 0.1 are the subgroup.
# covariates were generated from uniform distrition.
# y = 1-I-trt+2x_2+4x_4+3trt*I+eplison
# trt label were generated using logit(trt)=-4+3*x_1+5*x_3
# output is a dataframe including X (covariates), y (response), trt "treatment label", prtx (treatment label probility)
# ==============================================
gdataM <- function(n,depth, beta1, beta2){
NX  <- 4;
NPATIENT  <- n;
covariatesX <<- matrix(runif(NX*NPATIENT),nrow=NPATIENT);
expLogit  <- exp(-4+3*covariatesX[,1]+5*covariatesX[,3]);
treatmentProbT  <- expLogit/(1+expLogit);
#treatmentProbT  <- 0.5
treatmentT  <- rbinom(NPATIENT,1,treatmentProbT);
if(depth==1){
subGroupIndex  <- ( covariatesX[,1] < 0.5);
}else {
subGroupIndex  <- ( covariatesX[,1] > 0.3 & covariatesX[,3] > 0.1);
}
responseY1Mean  <- 1 + 2*covariatesX[,2] + 4*covariatesX[,4] + beta1*(subGroupIndex)*treatmentT
responseY1  <- responseY1Mean  +  rnorm(NPATIENT);
responseY0Mean  <- 1 + 2*covariatesX[,2] + 4*covariatesX[,4] + beta2*(1-subGroupIndex)*(1-treatmentT)
responseY0  <- responseY0Mean + rnorm(NPATIENT);
responseY  <- treatmentT*responseY1+(1-treatmentT)*responseY0
dataM  <- as.data.frame(cbind(covariatesX,responseY,treatmentT,treatmentProbT));
names(dataM)  <- c(paste("X",c(1:4), sep=""),"y","trt","prtx");
dataM
}
# ===========================================================================
# itrtest is used to generate unitlity correponding to each cut.
# dat is a dataframe
# dat$y outcome variable
# dat$trt treatment lable
# dat$prtx treatment assignment probability
# z is the new treatment assignment
# itr is defined as the mean of y where trt==z
# ===========================================================================
itrtest <- function(dat,z,n0){
y <- dat$y
trt <- dat$trt
prtx <- dat$prtx
itr <- NA
n <- nrow(dat)
if (length(y)!=length(z)) stop("the vector z must have the same length as data.")
if(n > n0) {
itr <- mean(trt*y*z/prtx+(1-trt)*y*(1-z)/(1-prtx))
}
itr
}
# ===========================================================================
# itrtest2 is used to generate unitlity correponding to each cut by the Zhou value function.
# dat is a dataframe
# dat$y outcome variable
# dat$trt treatment lable
# dat$prtx treatment assignment probability
# z is the new treatment assignment
# itr is defined as (1/n.1)*sum(z*trt*y/prtx)+(1/n.0)*sum(((1-z)*(1-trt)*y/prtx))
# ===========================================================================
itrtest2 <- function(dat,z,n0){
y <- dat$y
trt <- dat$trt
prtx <- dat$prtx
itr <- NA
n <- nrow(dat)
if (length(y)!=length(z)) stop("the vector z must have the same length as data.")
if(n > n0) {
n.0 = length(y[trt==0])
n.1 = n - n.0
itr <- (1/n.1)*sum(z*y/prtx)+(1/n.0)*sum(((1-z)*y/(1-prtx)))
}
itr
}
# ================================================================
# ONE SINGLE SPLIT USING ITR UTILITY FUNCTION FOR INTERACTION
# ================================================================
# WHEN USING FOR RANDOM FORESTS, SET test=NULL.
# min.ndsz= SETS THE MINIMUM NUMBER OF OBSERVATIONS FOR CLAIMING A TERMINAL NODE
# n0 = SETS THE MINIMUM NUMBER OF OBSERVATIONS FOR (n11, n10, n01, n00).
# n0: This is historical variable taking from IT tree. keep for convenience.
# split.var = ASKS FOR THE COLUMNS OF SPLITTING VARIABELS, INCLUDING BOTH CONTINUOUS AND CATEGORICAL ONES
# ctg = SPECIFIES THE COLUMNS OF CATEGORICAL VARIABLES
# max.depth = SPECIFIED THE MAXIMUM HEIGHT OF A TREE (ANOTHER WAY OF STOPPING THE GROWTH).
# mtry= SPECIFIES THE NUMBER OF COVARIATES IN THE RANDOMLY SELECTED SUBSETS FOR SPLITTING
partition.ITR <- function(dat, test=NULL, name="0", min.ndsz=20, n0=5, split.var, ctg=NULL, max.depth=15, mtry=length(split.var))
{
# inialize various variable
call <- match.call()
out <- match.call(expand = F)
out$info <- NULL
out$name.l <- NULL
out$name.r <- NULL
out$left <- NULL
out$right <- NULL
out$... <- NULL
# label the binary tree by 1 (left) and 2 (right).
name.l <- paste(name, 1, sep="")
name.r <- paste(name, 2, sep="")
# sample size
n <- nrow(dat)
# check whether testing data is provided
if (!is.null(test)) {
n.test <- nrow(test)
score.test <- NA
}
# prepare for the first cut these variable were used to store final cut information
var <- NA
vname <- NA
cut <- NA
# inilize score statistics.
# at the initial stage, there is no subgroup.
# Inidividuals either assign to trt=1 (z=rep(1,dim(dat)[1]))
# or trt=0 (z=rep(0,dim(dat)[1])) depending on which one gives better utility.
if(name==0){
max.score <- max(itrtest(dat, z=rep(0,dim(dat)[1]), n0),itrtest(dat, z=rep(1,dim(dat)[1]), n0))
}else{
max.score <- itrtest(dat, z=dat$new.trt, n0)
}
# extract value from data
trt <- dat$trt
y <- dat$y
vnames <- colnames(dat)
# COMPUTE THE TREATMENT EFFECT IN CURRENT NODE
trt.effect <- NA
n.0 = length(y[trt==0])
n.1 = n - n.0
if (min(n.1, n.0) >0) {
trt.effect <- mean(y[trt==1]) - mean(y[trt==0])
}
# CONTROL THE MAX TREE DEPTH
# name is the currently tree label.
# only when currently depth < max.depth and n > min terminal node proceed.
depth <- nchar(name)
if (depth <= max.depth && n >= min.ndsz) {  #this is probably not necessary
if (is.null(mtry)) {
m.try = length(split.var)
}else{
m.try = mtry
}
# if this is not random forrest, program will loop over all covariates.
for(i in sample(split.var, size=m.try, replace=F)) { #for us, split.var size is 4 and m.try is 4
x <- dat[,i]
v.name <- vnames[i]
temp <- sort(unique(x))
if(length(temp) > 1) {
# handle categorial variable first, otherwise take out the final value as no cut after it.
if (is.element(i,ctg)){
zcut <- power.set(temp)
} else{
zcut <- temp[-length(temp)]
}
# zcut are the values for all possible cut
for(j in zcut) {
score <- NA
if (is.element(i,ctg)){
grp <- sign(is.element(x, j))
cut1 <- paste(j, collapse=" ")
} else  {
# define left and right groups
grp.l <- sign(x <= j)
cut1.l <-  cbind("l",as.character(j))
grp.r <- sign(x > j)
cut1.r <- cbind("r",as.character(j))
}
# use itr rule to calcuate measure of splitting
n.1 <- sum(grp.l==1)
n.0 <- n-n.1
score.l <- itrtest2(dat, z=grp.l, n0)
n.1 <- sum(grp.r==1)
n.0 <- n-n.1
score.r <- itrtest2(dat, z=grp.r, n0)
# record the one with improved utility
if (!is.na(score.l) && !is.na(score.r)) {
if(score.l>max.score & score.r>max.score){
if(score.l>score.r) {
max.score <- score.l
var <- i
vname <- v.name
cut <- cut1.l
best.cut<-j
}else{
max.score <- score.r
var <- i
vname <- v.name
cut <- cut1.r
best.cut<-j
}
}else if(score.l>max.score & score.r<max.score){
max.score <- score.l
var <- i
vname <- v.name
cut <- cut1.l
best.cut<-j
}else if(score.l<max.score & score.r>max.score){
max.score <- score.r
var <- i
vname <- v.name
cut <- cut1.r
best.cut<-j
}
}
}
}
}
}
# when testing data is provided, assess new treatment assignment
# using testing sample and the rule caluclated from training sample
# var is the covariates calcualted before where spliting adopts.
# best.cut is the cutting point.
if (!is.null(test)) {
n.test <- nrow(test)
score.test <- NA
if (!is.na(var)) {
if (is.element(var,ctg)) {
grp.test <- sign(is.element(test[,var], best.cut))
}
else  {
grp.test <- sign(test[,var] <= best.cut)
}
score.test <- irttest(test, z=grp.test, n0=(n0/2))
if (!is.na(score.test)){
out$name.l <- name.l
out$name.r <- name.r
out$left.test <- test[grp.test==1,  ]
out$right.test <- test[grp.test==0,  ]
if (is.element(var,ctg)) {
out$left  <- dat[is.element(dat[,var], best.cut),]
out$right <- dat[!is.element(dat[,var], best.cut), ]}
else {
out$left  <- dat[dat[,var]<= best.cut,]
out$right <- dat[dat[,var]> best.cut, ]
}
} else {
var <- NA
vname <- NA
cut <- NA
max.score <- NA
}
# output results from both testing and training data.
out$info <- data.frame(node=name, size = n, n.1=n.1, n.0=n.0, trt.effect=trt.effect,var = var,
vname=vname, cut= cut, score=ifelse(max.score==-1e10, NA, max.score),score.test, size.test=n.test)
} else {
out$info <- data.frame(node=name, size = n, n.1=n.1, n.0=n.0, trt.effect=trt.effect, var = NA,
vname=NA, cut.1= NA,cut.2=NA, score=NA,score.test=NA, size.test=n.test)
}
}	else {
# if no testing data output results from training data only.
if (!is.na(var)) {
out$name.l <- name.l
out$name.r <- name.r
if (is.element(var,ctg)) {
out$left  <- dat[is.element(dat[,var], best.cut),]
out$right <- dat[!is.element(dat[,var], best.cut), ]}
else {
if(cut[1]=='l'){
out$left  <- cbind(dat[dat[,var]<= best.cut,],new.trt=rep(1,n=sum(dat[,var]<= best.cut)))
out$right <- cbind(dat[dat[,var]> best.cut, ],new.trt=rep(0,n=sum(dat[,var]> best.cut)))
}else{
out$left  <- cbind(dat[dat[,var]<= best.cut,],new.trt=rep(0,n=sum(dat[,var]<= best.cut)))
out$right <- cbind(dat[dat[,var]> best.cut, ],new.trt=rep(1,n=sum(dat[,var]> best.cut)))
}
}
out$info <- data.frame(node=name, size = n, n.1=n.1, n.0=n.0, trt.effect=trt.effect, var = var,
vname=vname, cut= cut, score=ifelse(max.score==-1e10, NA, max.score))
} else {
out$info <- data.frame(node=name, size = n, n.1=n.1, n.0=n.0, trt.effect=trt.effect,var=NA,
vname=NA, cut.1= NA,cut.2=NA, score=NA)
}
}
out
}
# =================================================
# THE grow.ITR() FUNCTION CONSTRUCTS A LARGE TREE
# =================================================
grow.ITR <- function(data, test=NULL, min.ndsz=20, n0=5, split.var, ctg=NULL, max.depth=15, mtry=length(split.var))
{
# initialize variables.
out <- NULL
list.nd <- NULL
list.test <- NULL
temp.list <- NULL
temp.test <- NULL
temp.name <- NULL
# record total dataset for spliting
list.nd <- list(data)
if (!is.null(test)) list.test <- list(test)
name <- 0
# loop over dataset for spliting
while (length(list.nd)!=0) {
for (i in 1:length(list.nd)){
if (!is.null(dim(list.nd[[i]])) && nrow(list.nd[[i]]) > 1){
test0 <- NULL
if (!is.null(test)) test0 <- list.test[[i]]
split <- partition.ITR(list.nd[[i]], test0, name[i], min.ndsz=min.ndsz,n0=n0, split.var=split.var, ctg=ctg, max.depth=max.depth, mtry=mtry)
out <- rbind(out, split$info)
if(!is.null(nrow(split$left))&&!is.null(nrow(split$right))){
min.n <- min(nrow(split$left),nrow(split$right))
}
if (!is.null(split$left) && min.n>min.ndsz && is.null(test)) {
temp.list <- c(temp.list, list(split$left, split$right))
temp.test <- c(temp.list, list(split$left, split$right))
temp.name <- c(temp.name, split$name.l, split$name.r)
} else if (!is.null(split$left) && min.n>min.ndsz && !is.null(test) && !is.null(split$left.test)) {
temp.list <- c(temp.list, list(split$left, split$right))
temp.name <- c(temp.name, split$name.l, split$name.r)
temp.test <- c(temp.test, list(split$left.test, split$right.test))
}
}
}
list.nd <- temp.list
list.test <- temp.test
name <- temp.name
temp.list <- NULL
temp.test <- NULL
temp.name <- NULL
}
out$node <- as.character(out$node)
out <- out[order(out$node), ]
out
}
data<-gdataM(n=1000, depth=2, beta1=2, beta2=1)
forest<-Build.RF.ITR(dat=data,col.y = "y", col.trt = "trt", col.prtx = "prtx",split.var = 1:4, ntree=100)
VI<-Variable.Importance.ITR(VI)
VI<-Variable.Importance.ITR(forest)
plot.VI(VI)
data<-gdataM(n=3000, depth=2, beta1=2, beta2=1)
forest<-Build.RF.ITR(dat=data,col.y = "y", col.trt = "trt", col.prtx = "prtx",split.var = 1:4, ntree=75)
VI<-Variable.Importance.ITR(forest)
